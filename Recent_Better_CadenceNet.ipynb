{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PinakW/DIssertation_expt/blob/main/Recent_Better_CadenceNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "BCeppY1fBRsz"
      },
      "outputs": [],
      "source": [
        "USE_ORIGINAL = 0\n",
        "loss = 'categorical_crossentropy'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "IYvbodAaO5NT"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "#For plotting the dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "#Data pipeline preparation\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "#model building\n",
        "from tensorflow.keras import models\n",
        "import tensorflow.keras.utils as tfutils\n",
        "import os\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "bJSJfW_tPA88"
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES = 10\n",
        "\n",
        "DataSet = 'cifar10'\n",
        "#'caltech101'\n",
        "#'cifar10'\n",
        "def num_samples_per_class(ds_train, get_top_10 = False, print_all = False):\n",
        "    vals = np.unique(np.fromiter(ds_train.map(lambda x, y: y), int), return_counts=True)\n",
        "    class_list = []\n",
        "    class_hist = []\n",
        "    for val,count in zip(*vals):\n",
        "        if print_all==True:\n",
        "            print(int(val), count)\n",
        "        class_hist.append((val,count))\n",
        "    if get_top_10 == True:\n",
        "        sorted_tuple = sorted(class_hist, key=lambda t: t[-1], reverse=True)[:(NUM_CLASSES + 1)]    #+1 because we are going to remove \"backround_google\" i.e. 4\n",
        "        class_list = [x for x,y in sorted_tuple]\n",
        "    return class_list\n",
        "\n",
        "def filter_fn(x, allowed_classes:list):\n",
        "    allowed_classes = tf.constant(allowed_classes)\n",
        "    isallowed = tf.equal(allowed_classes, tf.cast(x, allowed_classes.dtype))\n",
        "    reduced_sum = tf.reduce_sum(tf.cast(isallowed, tf.float32))\n",
        "    return tf.greater(reduced_sum, tf.constant(0.))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "2fLIbNS2PDZ1"
      },
      "outputs": [],
      "source": [
        "#ds_train = tfds.load(DataSet, split='train + test[:75%]', as_supervised=True)\n",
        "if DataSet == 'caltech101':\n",
        "  ds_train, train_info = tfds.load(DataSet, split='train + test[:75%]', as_supervised=True, with_info = True)\n",
        "else:\n",
        "  ds_train, train_info = tfds.load(DataSet, split='train', as_supervised=True, with_info = True)\n",
        "ds_test = tfds.load(DataSet, split='test', as_supervised=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "QOoGq7JtPGn8",
        "outputId": "24809d47-9235-46d9-9bb0-df8f6400e220"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['airplane',\n",
              " 'automobile',\n",
              " 'bird',\n",
              " 'cat',\n",
              " 'deer',\n",
              " 'dog',\n",
              " 'frog',\n",
              " 'horse',\n",
              " 'ship',\n",
              " 'truck']"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "class_list = num_samples_per_class(ds_train, get_top_10=True)\n",
        "if DataSet == 'caltech101':\n",
        "  class_list = [i for i in class_list if i != train_info.features['label'].str2int('background_google')]\n",
        "  class_list.sort()\n",
        "\n",
        "\"\"\"for name in train_info.features['label'].names:\n",
        "    print(name, train_info.features['label'].str2int(name))\n",
        "\"\"\"\n",
        "\n",
        "class_names = [train_info.features['label'].int2str(i) for i in class_list]\n",
        "display(class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "VeThcLypHU4m"
      },
      "outputs": [],
      "source": [
        "resized_ds_train = ds_train.filter(lambda x, y: filter_fn(y, class_list)) # as_supervised\n",
        "resized_ds_test = ds_test.filter(lambda x, y: filter_fn(y, class_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DApyIbKhPISb",
        "outputId": "58b6bbb2-728d-43cb-c0f6-a8fee959766b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 5000\n",
            "1 5000\n",
            "2 5000\n",
            "3 5000\n",
            "4 5000\n",
            "5 5000\n",
            "6 5000\n",
            "7 5000\n",
            "8 5000\n",
            "9 5000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "num_samples_per_class(resized_ds_train, print_all=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8z2tJJ3PLsE",
        "outputId": "d55b9ad1-e5f7-4d51-8389-b98ccf779132"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1000\n",
            "1 1000\n",
            "2 1000\n",
            "3 1000\n",
            "4 1000\n",
            "5 1000\n",
            "6 1000\n",
            "7 1000\n",
            "8 1000\n",
            "9 1000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "num_samples_per_class(resized_ds_test, print_all=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "QdPKLGVdPNrk"
      },
      "outputs": [],
      "source": [
        "#Hyperparameters\n",
        "if DataSet=='caltech101':\n",
        "    IMG_SIZE = 60\n",
        "elif DataSet=='cifar10':\n",
        "    IMG_SIZE = 32\n",
        "NUM_CHANNELS = 3\n",
        "BATCH_SIZE=128\n",
        "\n",
        "input_shape = (IMG_SIZE,IMG_SIZE,NUM_CHANNELS)\n",
        "#Relabelling to avoid issues. Note that human readability is reduced by this\n",
        "table = tf.lookup.StaticHashTable(\n",
        "    initializer=tf.lookup.KeyValueTensorInitializer(\n",
        "        keys=tf.constant(class_list, dtype=tf.int64),\n",
        "        #values=tf.constant([tfutils.to_categorical(0, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(1, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(2, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(3, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(4, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(5, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(6, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(7, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(8, num_classes=NUM_CLASSES, dtype=np.int64), tfutils.to_categorical(9, num_classes=NUM_CLASSES, dtype=np.int64)],  dtype=tf.int64),\n",
        "        values=tf.constant([0, 1, 2, 3, 4, 5, 6, 7, 8, 9],  dtype=tf.int64)\n",
        "    ),\n",
        "    default_value= tf.constant(0,  dtype=tf.int64)\n",
        ")\n",
        "\n",
        "#This function will be used in the graph execution hence @tf.function prefix\n",
        "@tf.function\n",
        "def map_func(label):\n",
        "    global class_list\n",
        "    global loss\n",
        "    mapped_label = table.lookup(label)\n",
        "    if loss != 'sparse_categorical_crossentropy':\n",
        "        mapped_label = tf.one_hot(indices=mapped_label, depth=NUM_CLASSES)\n",
        "    print(\"Label = \" + str(label) + \"\\t\" + \"Mapped Label = \" + str(mapped_label))\n",
        "    return mapped_label\n",
        "\n",
        "#Preprocessing done as part of the graph\n",
        "resize_and_rescale = tf.keras.Sequential([\n",
        "  layers.Resizing(IMG_SIZE, IMG_SIZE),\n",
        "  layers.Rescaling(1./255)\n",
        "])\n",
        "\n",
        "resize_layer = tf.keras.Sequential([\n",
        "  layers.Resizing(IMG_SIZE, IMG_SIZE),\n",
        "])\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.RandomRotation(0.2),\n",
        "])\n",
        "\n",
        "buffer_size = 30*NUM_CLASSES\n",
        "\n",
        "#Preprocessing function which invokes above graphs\n",
        "def prepare(ds, shuffle=False, augment=False, resize_only = False):\n",
        "    global buffer_size\n",
        "    global BATCH_SIZE\n",
        "    \n",
        "\n",
        "    # Resize and rescale all datasets.\n",
        "    if resize_only==True:\n",
        "        ds = ds.map(lambda x, y: (resize_layer(x), map_func(y)), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    else:\n",
        "        ds = ds.map(lambda x, y: (resize_and_rescale(x), map_func(y)), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    \n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size)\n",
        "        \n",
        "    # Batch all datasets.\n",
        "    #ds = ds.batch(BATCH_SIZE)\n",
        "\n",
        "    # Use data augmentation only on the training set.\n",
        "    if augment:\n",
        "        #f_ds = ds.filter(lambda x, y: filter_fn(y, [2,3,6]))    #[2,3,6] are the examples with lesser data. We are trying to bring back balance\n",
        "        #f_ds_aug = f_ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        #ds = ds.concatenate(f_ds_aug)\n",
        "        #ds_aug = ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        #ds = ds.concatenate(ds_aug)\n",
        "        ds_aug = ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        ds = ds.concatenate(ds_aug)\n",
        "\n",
        "        \n",
        "    # Use buffered prefetching on all datasets.\n",
        "    return ds.prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmLFCHD6PgLw",
        "outputId": "dcf09ff8-df93-4ad2-a085-5ab9550b0101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label = Tensor(\"label:0\", shape=(), dtype=int64)\tMapped Label = Tensor(\"one_hot:0\", shape=(10,), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "resized_ds_train = prepare(resized_ds_train, augment=True)\n",
        "resized_ds_test = prepare(resized_ds_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "eBn0xUIRPizz",
        "outputId": "dc7360bf-6254-4b04-98db-16d87adbd3a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], dtype=float32)>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=int64, numpy=7>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "for example in resized_ds_train.take(1):\n",
        "  plt.imshow(example[0])\n",
        "  display((example[-1]))\n",
        "  display(tf.argmax(example[-1]))\n",
        "  #display(train_info.features['label'].int2str(example[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "uf1KScmu9odE"
      },
      "outputs": [],
      "source": [
        "def num_samples_per_class_onehot(resized_ds_train, print_all=False):\n",
        "    if loss != 'sparse_categorical_crossentropy':\n",
        "        vals = np.unique(np.fromiter(resized_ds_train.map(lambda x, y: tf.argmax(y)), int), return_counts=True)\n",
        "    else:\n",
        "        vals = np.unique(np.fromiter(resized_ds_train.map(lambda x, y: y), int), return_counts=True)\n",
        "    class_list = []\n",
        "    class_hist = []\n",
        "    for val,count in zip(*vals):\n",
        "        if print_all==True:\n",
        "            print(int(val), count)\n",
        "        class_hist.append((val,count))\n",
        "    class_hist.sort()\n",
        "    return class_hist\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "avZ2eG0Ty3fi",
        "outputId": "1cc62c7a-6dec-4699-fb43-ca30990ae9a5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[(0, 10000),\n",
              " (1, 10000),\n",
              " (2, 10000),\n",
              " (3, 10000),\n",
              " (4, 10000),\n",
              " (5, 10000),\n",
              " (6, 10000),\n",
              " (7, 10000),\n",
              " (8, 10000),\n",
              " (9, 10000)]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Post prepare function, all the labels will be converted to one hot encoders. In order to get class-wise distribution, we will need to convert each one hot encoder into its label (temporarily)\n",
        "#We need a new function to handle it\n",
        "class_hist = num_samples_per_class_onehot(resized_ds_train)\n",
        "display(class_hist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "ftnZ5OyvQB98",
        "outputId": "0bbad16c-87d2-4828-e112-c13d69116ff0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Better CadenceNet'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Better CadenceNet'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#reg = tf.keras.regularizers.L2(0.01)\n",
        "reg = tf.keras.regularizers.L1L2(l1 =0.0, l2 = 0.1)\n",
        "#reg = tf.keras.regularizers.L1L2(l1 =0.0, l2 = 0.0)\n",
        "#beta_regularizer = 0.1\n",
        "#gamma_regularizer = 0.1\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "if USE_ORIGINAL == 1:\n",
        "\t\tdisplay(\"Original CadenceNet\")\n",
        "\t\t#model.add(resize_and_rescale)\n",
        "\t\t#model.add(data_augmentation)\n",
        "\t\tkernel_size = (5,5)\n",
        "\t\tmodel.add(layers.Conv2D(64, kernel_size, input_shape = input_shape, padding=\"same\", kernel_regularizer = reg))       #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tpool_size = (2,2)\n",
        "\t\tmodel.add(layers.MaxPool2D(pool_size))\n",
        "\t\tmodel.add(layers.Dropout(.2))\n",
        "\n",
        "\t\tkernel_size = (3,3)\n",
        "\t\tmodel.add(layers.Conv2D(192, kernel_size, padding=\"same\", kernel_regularizer = reg))      #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())                                                      #beta_regularizer = beta_regularizer, gamma_regularizer = gamma_regularizer\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tpool_size = (2,2)\n",
        "\t\tmodel.add(layers.MaxPool2D(pool_size))\n",
        "\t\tmodel.add(layers.Dropout(.2))\n",
        "\t#model.add(layers.SpatialDropout2D(0.2))\n",
        "\t\n",
        "\t\tkernel_size = (3,3)\n",
        "\t\tmodel.add(layers.Conv2D(64, kernel_size, padding=\"same\", kernel_regularizer = reg))       #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tmodel.add(layers.Dropout(.2))\n",
        "\t\n",
        "\t\tkernel_size = (3,3)\n",
        "\t\tmodel.add(layers.Conv2D(128, kernel_size, padding=\"same\", kernel_regularizer = reg))      #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tpool_size = (2,2)\n",
        "\t\tmodel.add(layers.MaxPool2D(pool_size))\n",
        "\t\tmodel.add(layers.Dropout(.2))\n",
        "\t\t#model.add(layers.SpatialDropout2D(0.2))\n",
        "\t\n",
        "\t\tmodel.add(layers.Flatten())\n",
        "\t\tmodel.add(layers.Dropout(.2))\n",
        "\t\t#model.add(layers.Dense(NUM_CLASSES, activation='softmax', kernel_regularizer = reg))\n",
        "\t\tmodel.add(layers.Dense(NUM_CLASSES, kernel_regularizer = reg))\n",
        "\t\tmodel.add(layers.Softmax())\n",
        "else:\n",
        "\t\tdisplay(\"Better CadenceNet\")\n",
        "\t\tkernel_size = (3,3)\n",
        "\t\tmodel.add(layers.Conv2D(32, kernel_size, input_shape = input_shape, padding=\"same\", kernel_regularizer = reg))       #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tpool_size = (2,2)\n",
        "\t\tmodel.add(layers.MaxPool2D(pool_size))\n",
        "\t\t#model.add(layers.SpatialDropout2D(0.2))\n",
        "\t\n",
        "\t\tkernel_size = (3,3)\n",
        "\t\tmodel.add(layers.Conv2D(64, kernel_size, padding=\"same\", kernel_regularizer = reg))      #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tpool_size = (2,2)\n",
        "\t\tmodel.add(layers.SpatialDropout2D(0.1))\n",
        "\t\n",
        "\t\tkernel_size = (3,3)\n",
        "\t\tmodel.add(layers.Conv2D(128, kernel_size, padding=\"same\", kernel_regularizer = reg))       #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tpool_size = (2,2)\n",
        "\t\t#model.add(layers.SpatialDropout2D(0.2))\n",
        "\n",
        "\n",
        "#\t\tmodel.add(layers.MaxPool2D(pool_size))\n",
        "\t\n",
        "\t\t#kernel_size = (3,3)\n",
        "\t\t#model.add(layers.Conv2D(192, kernel_size, padding=\"same\", kernel_regularizer = reg))      #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\t#model.add(layers.BatchNormalization())\n",
        "\t\t#model.add(layers.ReLU())\n",
        "\t\t#pool_size = (2,2)\n",
        "\t\t#model.add(layers.MaxPool2D(pool_size))\n",
        "\n",
        "\t\tkernel_size = (3,3)\n",
        "\t\tmodel.add(layers.Conv2D(64, kernel_size, padding=\"same\", kernel_regularizer = reg))      #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tpool_size = (2,2)\n",
        "#\t\tmodel.add(layers.MaxPool2D(pool_size))\n",
        "\t\t\n",
        "\t\tkernel_size = (3,3)\n",
        "\t\tmodel.add(layers.Conv2D(32, kernel_size, padding=\"same\", kernel_regularizer = reg))      #TODO: For instance, after a Conv2D layer with data_format=\"channels_first\", set axis=1 in BatchNormalization.\n",
        "\t\tmodel.add(layers.BatchNormalization())\n",
        "\t\tmodel.add(layers.ReLU())\n",
        "\t\tpool_size = (2,2)\n",
        "\t\tmodel.add(layers.MaxPool2D(pool_size))\n",
        "\t\n",
        "\t\tmodel.add(layers.Flatten())\n",
        "\t\t#model.add(layers.Dropout(.2))\n",
        "\t\t#model.add(layers.Dense(1000, kernel_regularizer = reg))\n",
        "\t\t#model.add(layers.Dropout(.02))\n",
        "\t\tmodel.add(layers.Dense(NUM_CLASSES, kernel_regularizer = reg))\n",
        "\t\tmodel.add(layers.Softmax())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "B-aANPwedhNH"
      },
      "outputs": [],
      "source": [
        "def get_class_weights(class_hist):\n",
        "    \"\"\"\n",
        "    Returns the class weights as a tf.Tensor. Class weights are inverse of the class frequencies\n",
        "    Class frequencies are the number of samples of each class which we calculate in earlier steps\n",
        "    \"\"\"\n",
        "    inv_freq = tf.convert_to_tensor([1.0/count for label, count in class_hist], dtype=tf.float32)\n",
        "    return tfutils.normalize(inv_freq)\n",
        "\n",
        "\n",
        "def weightedloss(y_true, y_pred, gamma, class_weight):\n",
        "    \"\"\"\n",
        "    We assume that all arguments coming into this function are tf.Tensors type\n",
        "    class_weights are basically alpha in focal loss paper\n",
        "    \"\"\"\n",
        "    #ones = tf.convert_to_tensor(np.ones(shape=len(y_true)))\n",
        "    a = tf.math.multiply(tf.math.pow(tf.math.subtract(1.0, y_pred), gamma), tf.math.log(y_pred))  #((1-pt)^gamma)log(pt)\n",
        "    b = tf.math.multiply(-1.0, class_weight)                                                          #-alpha\n",
        "    b = tf.math.multiply(b,a)    \n",
        "    b = tf.math.multiply(b, y_true)\n",
        "    return b\n",
        "class WeightedLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, gamma, class_weight=np.ones(shape=NUM_CLASSES, dtype=np.float32)):\n",
        "        super().__init__()\n",
        "        self.gamma = tf.convert_to_tensor(gamma)\n",
        "        self.class_weight = tf.convert_to_tensor(class_weight, dtype=tf.float32)\n",
        "    def call(self, y_true, y_pred):\n",
        "        return weightedloss(y_true, y_pred, self.gamma, self.class_weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "fHRlWtV83IeV"
      },
      "outputs": [],
      "source": [
        "Learning_Rate = 1e-5\n",
        "#tf.keras.optimizers.Adam(learning_rate=Learning_Rate)     #OR tf.keras.optimizers.SGD(learning_rate=Learning_Rate, momentum=0.0)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=Learning_Rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "DHFaNgqtwZGz"
      },
      "outputs": [],
      "source": [
        "###EITHER\n",
        "\n",
        "#!pip install focal-loss\n",
        "#from focal_loss import SparseCategoricalFocalLoss \n",
        "#model.compile( optimizer = opt, loss = SparseCategoricalFocalLoss(gamma=2), metrics=['accuracy'] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "nFFBTJNwLAcA"
      },
      "outputs": [],
      "source": [
        "###OR\n",
        "#model.compile( optimizer = opt, loss = loss, metrics=['accuracy'] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "pmqZtduVbl-2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "0712cb76-eba1-4bda-e8aa-13362aad9397"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
              "array([[0.31622776, 0.31622776, 0.31622776, 0.31622776, 0.31622776,\n",
              "        0.31622776, 0.31622776, 0.31622776, 0.31622776, 0.31622776]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "###OR\n",
        "class_wts = get_class_weights(class_hist)\n",
        "display(class_wts)\n",
        "model.compile( optimizer = opt, loss = WeightedLoss(gamma=2.0, class_weight=class_wts), metrics=['accuracy'] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "HL7YFZKvbn92"
      },
      "outputs": [],
      "source": [
        "#model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKoB4BcEQVkU",
        "outputId": "c56b6ffb-bc40-44ef-e442-66d49fc8b5df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/80\n",
            "782/782 [==============================] - 52s 59ms/step - loss: 23.8046 - accuracy: 0.1240 - val_loss: 19.7441 - val_accuracy: 0.1196\n",
            "Epoch 2/80\n",
            "782/782 [==============================] - 39s 50ms/step - loss: 16.4992 - accuracy: 0.1556 - val_loss: 13.5734 - val_accuracy: 0.1548\n",
            "Epoch 3/80\n",
            "782/782 [==============================] - 41s 52ms/step - loss: 11.2281 - accuracy: 0.1953 - val_loss: 9.1180 - val_accuracy: 0.1991\n",
            "Epoch 4/80\n",
            "782/782 [==============================] - 39s 50ms/step - loss: 7.4391 - accuracy: 0.2345 - val_loss: 5.9386 - val_accuracy: 0.2377\n",
            "Epoch 5/80\n",
            "782/782 [==============================] - 39s 50ms/step - loss: 4.7636 - accuracy: 0.2721 - val_loss: 3.7258 - val_accuracy: 0.2675\n",
            "Epoch 6/80\n",
            "782/782 [==============================] - 40s 51ms/step - loss: 2.9321 - accuracy: 0.3043 - val_loss: 2.2429 - val_accuracy: 0.2881\n",
            "Epoch 7/80\n",
            "782/782 [==============================] - 41s 52ms/step - loss: 1.7313 - accuracy: 0.3352 - val_loss: 1.2981 - val_accuracy: 0.2960\n",
            "Epoch 8/80\n",
            "782/782 [==============================] - 40s 51ms/step - loss: 0.9861 - accuracy: 0.3627 - val_loss: 0.7313 - val_accuracy: 0.2993\n",
            "Epoch 9/80\n",
            "782/782 [==============================] - 41s 53ms/step - loss: 0.5513 - accuracy: 0.3885 - val_loss: 0.4125 - val_accuracy: 0.3078\n",
            "Epoch 10/80\n",
            "782/782 [==============================] - 41s 53ms/step - loss: 0.3130 - accuracy: 0.4071 - val_loss: 0.2433 - val_accuracy: 0.2999\n",
            "Epoch 11/80\n",
            "782/782 [==============================] - 41s 52ms/step - loss: 0.1877 - accuracy: 0.4121 - val_loss: 0.1543 - val_accuracy: 0.3153\n",
            "Epoch 12/80\n",
            "782/782 [==============================] - 40s 52ms/step - loss: 0.1231 - accuracy: 0.4174 - val_loss: 0.1093 - val_accuracy: 0.3149\n",
            "Epoch 13/80\n",
            "782/782 [==============================] - 41s 52ms/step - loss: 0.0906 - accuracy: 0.4180 - val_loss: 0.0870 - val_accuracy: 0.3144\n",
            "Epoch 14/80\n",
            "782/782 [==============================] - 42s 53ms/step - loss: 0.0740 - accuracy: 0.4203 - val_loss: 0.0755 - val_accuracy: 0.3137\n",
            "Epoch 15/80\n",
            "782/782 [==============================] - 40s 52ms/step - loss: 0.0649 - accuracy: 0.4223 - val_loss: 0.0693 - val_accuracy: 0.3068\n",
            "Epoch 16/80\n",
            "782/782 [==============================] - 40s 52ms/step - loss: 0.0592 - accuracy: 0.4229 - val_loss: 0.0635 - val_accuracy: 0.3209\n",
            "Epoch 17/80\n",
            "782/782 [==============================] - 40s 51ms/step - loss: 0.0555 - accuracy: 0.4243 - val_loss: 0.0612 - val_accuracy: 0.3129\n",
            "Epoch 18/80\n",
            "782/782 [==============================] - 40s 51ms/step - loss: 0.0530 - accuracy: 0.4250 - val_loss: 0.0594 - val_accuracy: 0.3089\n",
            "Epoch 19/80\n",
            "782/782 [==============================] - 41s 52ms/step - loss: 0.0513 - accuracy: 0.4255 - val_loss: 0.0577 - val_accuracy: 0.3208\n",
            "Epoch 20/80\n",
            "782/782 [==============================] - 41s 52ms/step - loss: 0.0501 - accuracy: 0.4260 - val_loss: 0.0560 - val_accuracy: 0.3200\n",
            "Epoch 21/80\n",
            "782/782 [==============================] - 40s 51ms/step - loss: 0.0491 - accuracy: 0.4286 - val_loss: 0.0557 - val_accuracy: 0.3193\n",
            "Epoch 22/80\n",
            "782/782 [==============================] - 40s 51ms/step - loss: 0.0483 - accuracy: 0.4277 - val_loss: 0.0554 - val_accuracy: 0.3122\n",
            "Epoch 23/80\n",
            "782/782 [==============================] - 41s 52ms/step - loss: 0.0477 - accuracy: 0.4272 - val_loss: 0.0548 - val_accuracy: 0.3107\n",
            "Epoch 24/80\n",
            "782/782 [==============================] - 40s 51ms/step - loss: 0.0472 - accuracy: 0.4276 - val_loss: 0.0544 - val_accuracy: 0.3123\n",
            "Epoch 25/80\n",
            "782/782 [==============================] - 39s 50ms/step - loss: 0.0468 - accuracy: 0.4269 - val_loss: 0.0548 - val_accuracy: 0.3099\n",
            "Epoch 26/80\n",
            "782/782 [==============================] - 41s 53ms/step - loss: 0.0463 - accuracy: 0.4285 - val_loss: 0.0535 - val_accuracy: 0.3143\n",
            "Epoch 27/80\n",
            "782/782 [==============================] - 39s 50ms/step - loss: 0.0460 - accuracy: 0.4280 - val_loss: 0.0542 - val_accuracy: 0.3001\n",
            "Epoch 28/80\n",
            "782/782 [==============================] - 41s 52ms/step - loss: 0.0457 - accuracy: 0.4284 - val_loss: 0.0525 - val_accuracy: 0.3188\n",
            "Epoch 29/80\n",
            "782/782 [==============================] - 40s 51ms/step - loss: 0.0454 - accuracy: 0.4288 - val_loss: 0.0541 - val_accuracy: 0.3055\n",
            "Epoch 30/80\n",
            "782/782 [==============================] - 40s 51ms/step - loss: 0.0452 - accuracy: 0.4304 - val_loss: 0.0541 - val_accuracy: 0.3048\n",
            "Epoch 31/80\n",
            "782/782 [==============================] - 41s 52ms/step - loss: 0.0449 - accuracy: 0.4281 - val_loss: 0.0529 - val_accuracy: 0.3053\n",
            "Epoch 32/80\n",
            "782/782 [==============================] - 40s 51ms/step - loss: 0.0448 - accuracy: 0.4293 - val_loss: 0.0529 - val_accuracy: 0.3084\n",
            "Epoch 33/80\n",
            "782/782 [==============================] - 39s 50ms/step - loss: 0.0446 - accuracy: 0.4291 - val_loss: 0.0522 - val_accuracy: 0.3138\n",
            "Epoch 34/80\n",
            "782/782 [==============================] - 39s 50ms/step - loss: 0.0444 - accuracy: 0.4318 - val_loss: 0.0529 - val_accuracy: 0.3058\n",
            "Epoch 35/80\n",
            "782/782 [==============================] - 40s 51ms/step - loss: 0.0442 - accuracy: 0.4324 - val_loss: 0.0515 - val_accuracy: 0.3161\n",
            "Epoch 36/80\n",
            "782/782 [==============================] - 40s 51ms/step - loss: 0.0440 - accuracy: 0.4336 - val_loss: 0.0526 - val_accuracy: 0.3061\n",
            "Epoch 37/80\n",
            "782/782 [==============================] - 39s 50ms/step - loss: 0.0439 - accuracy: 0.4331 - val_loss: 0.0523 - val_accuracy: 0.3092\n",
            "Epoch 38/80\n",
            "782/782 [==============================] - 39s 50ms/step - loss: 0.0437 - accuracy: 0.4338 - val_loss: 0.0523 - val_accuracy: 0.3092\n",
            "Epoch 39/80\n",
            "782/782 [==============================] - 40s 51ms/step - loss: 0.0436 - accuracy: 0.4352 - val_loss: 0.0522 - val_accuracy: 0.3089\n",
            "Epoch 40/80\n",
            "782/782 [==============================] - 39s 50ms/step - loss: 0.0435 - accuracy: 0.4338 - val_loss: 0.0535 - val_accuracy: 0.3016\n",
            "Epoch 41/80\n",
            "782/782 [==============================] - 39s 50ms/step - loss: 0.0433 - accuracy: 0.4351 - val_loss: 0.0512 - val_accuracy: 0.3144\n",
            "Epoch 42/80\n",
            "782/782 [==============================] - 40s 51ms/step - loss: 0.0432 - accuracy: 0.4349 - val_loss: 0.0519 - val_accuracy: 0.3088\n",
            "Epoch 43/80\n",
            "782/782 [==============================] - 39s 50ms/step - loss: 0.0431 - accuracy: 0.4354 - val_loss: 0.0525 - val_accuracy: 0.3069\n",
            "Epoch 44/80\n",
            "782/782 [==============================] - 39s 50ms/step - loss: 0.0430 - accuracy: 0.4370 - val_loss: 0.0539 - val_accuracy: 0.2939\n",
            "Epoch 45/80\n",
            "782/782 [==============================] - 41s 52ms/step - loss: 0.0429 - accuracy: 0.4374 - val_loss: 0.0533 - val_accuracy: 0.2973\n",
            "Epoch 46/80\n",
            "782/782 [==============================] - 40s 51ms/step - loss: 0.0427 - accuracy: 0.4389 - val_loss: 0.0529 - val_accuracy: 0.3013\n",
            "Epoch 47/80\n",
            "782/782 [==============================] - 39s 50ms/step - loss: 0.0427 - accuracy: 0.4378 - val_loss: 0.0520 - val_accuracy: 0.3076\n",
            "Epoch 48/80\n",
            "782/782 [==============================] - 39s 50ms/step - loss: 0.0426 - accuracy: 0.4383 - val_loss: 0.0523 - val_accuracy: 0.3048\n",
            "Epoch 49/80\n",
            "782/782 [==============================] - 40s 51ms/step - loss: 0.0425 - accuracy: 0.4399 - val_loss: 0.0513 - val_accuracy: 0.3123\n",
            "Epoch 50/80\n",
            "782/782 [==============================] - 39s 50ms/step - loss: 0.0423 - accuracy: 0.4417 - val_loss: 0.0533 - val_accuracy: 0.3031\n",
            "Epoch 51/80\n",
            "782/782 [==============================] - 39s 50ms/step - loss: 0.0422 - accuracy: 0.4427 - val_loss: 0.0513 - val_accuracy: 0.3120\n",
            "Epoch 52/80\n",
            "782/782 [==============================] - 46s 59ms/step - loss: 0.0422 - accuracy: 0.4411 - val_loss: 0.0508 - val_accuracy: 0.3200\n",
            "Epoch 53/80\n",
            "597/782 [=====================>........] - ETA: 7s - loss: 0.0416 - accuracy: 0.4495"
          ]
        }
      ],
      "source": [
        "#h = model.fit( resized_ds_train, epochs=10)\n",
        "resized_ds_train = resized_ds_train.batch(BATCH_SIZE)\n",
        "resized_ds_test_unbatched = resized_ds_test\n",
        "resized_ds_test = resized_ds_test.batch(BATCH_SIZE)\n",
        "\n",
        "h = model.fit( resized_ds_train, epochs=80, validation_data = resized_ds_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuOyiBsTQkYL"
      },
      "outputs": [],
      "source": [
        "plt.plot(h.history['loss'])\n",
        "plt.plot(h.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(h.history['accuracy'])\n",
        "plt.plot(h.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_35FPMMk5yd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxSSK5SPhnKL"
      },
      "outputs": [],
      "source": [
        "#Evaluation and confusion matrix creation:\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "x_test = np.asarray(list(map(lambda x: x[0], tfds.as_numpy(resized_ds_test_unbatched))))\n",
        "y_test_orig = np.asarray(list(map(lambda x: x[1], tfds.as_numpy(resized_ds_test_unbatched))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tg4EdPBuc7fW"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYYhORws1NnZ"
      },
      "outputs": [],
      "source": [
        "if loss!='sparse_categorical_crossentropy':\n",
        "    false_arr = np.full(shape=len(class_list), fill_value = False)\n",
        "    #y_pred = np.empty(shape=y_test_orig.shape[-1])\n",
        "    i=0\n",
        "    for i, pred in enumerate(predictions):\n",
        "        temp_arr = copy.deepcopy(false_arr)\n",
        "        np.put(temp_arr, np.argmax(pred), True)\n",
        "        if i==0:\n",
        "            y_pred = copy.deepcopy(temp_arr)\n",
        "        else:\n",
        "            y_pred = np.vstack([y_pred, temp_arr])\n",
        "    display(y_pred.shape)\n",
        "else:\n",
        "    y_pred = np.argmax(predictions, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-iQ19WTaE9s"
      },
      "outputs": [],
      "source": [
        "display(y_test_orig.shape)\n",
        "display(y_pred.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Go8FAKdprJVD"
      },
      "outputs": [],
      "source": [
        "print('Confusion Matrix')\n",
        "if loss != 'sparse_categorical_crossentropy':\n",
        "    matrix = confusion_matrix(y_test_orig.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "else:\n",
        "    matrix = confusion_matrix(y_test_orig, y_pred)\n",
        "display(matrix)\n",
        "\n",
        "# Print Classification Report\n",
        "print('Classification Report')\n",
        "print(classification_report(y_test_orig, y_pred, target_names=class_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5FRx9tVhibX"
      },
      "source": [
        "NOT using below things"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NaFdDuTQoyT"
      },
      "outputs": [],
      "source": [
        "def ret_as_numpy():\n",
        "    test = tfds.load(DataSet, split='test', as_supervised=True)\n",
        "    test = prepare(test)\n",
        "    test = tfds.as_numpy(test)\n",
        "    return test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Si_MguzMQuZL"
      },
      "outputs": [],
      "source": [
        "test_as_np = ret_as_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWYWlODgQrFy"
      },
      "outputs": [],
      "source": [
        "def evaluate_float_model(model, test):\n",
        "    test_labels = []\n",
        "    \n",
        "    # Run predictions on every image in the \"test\" dataset.\n",
        "    prediction_digits = []\n",
        "    for i, test_example in enumerate(test):\n",
        "        if i % 1000 == 0:\n",
        "            print('Evaluated on {n} results so far.'.format(n=i))\n",
        "        test_labels.append(np.argmax(test_example[-1]))\n",
        "        test_image = test_example[0]\n",
        "        # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "        # the model's input data format.\n",
        "        #display(test_image.shape)\n",
        "        test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "        #test_image = np.expand_dims(test_image, axis=3).astype(np.float32)\n",
        "        #display(test_image.shape)\n",
        "        \n",
        "        # Run inference.\n",
        "        output = model(test_image, training=False)\n",
        "        # Post-processing: remove batch dimension and find the digit with highest\n",
        "        # probability.\n",
        "        output = output.numpy()\n",
        "        #display(output[0])\n",
        "        digit = np.argmax(output[0])\n",
        "        prediction_digits.append(digit)\n",
        "        \n",
        "    print('\\n')\n",
        "    #display(output[0])\n",
        "    #display(output)\n",
        "    #display(digit)\n",
        "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "    #display(prediction_digits)\n",
        "    #display(test_labels)\n",
        "    prediction_digits = np.array(prediction_digits)\n",
        "    accuracy = (prediction_digits == test_labels).mean()\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOHIU_J3QxE7"
      },
      "outputs": [],
      "source": [
        "test_accuracy_Float = evaluate_float_model(model, test_as_np)\n",
        "\n",
        "print('Float test_accuracy:', test_accuracy_Float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_Q2Is9IY-Oo"
      },
      "source": [
        "Float checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fr6QAx7Qztb"
      },
      "outputs": [],
      "source": [
        "! pip install -q tensorflow-model-optimization\n",
        "import tensorflow_model_optimization as tfmot\n",
        "quantize_model = tfmot.quantization.keras.quantize_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfG_qAU4Q3DL"
      },
      "outputs": [],
      "source": [
        "q_aware_model = quantize_model(model)\n",
        "q_aware_model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
        "q_aware_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYdW-VK8Q5mT"
      },
      "outputs": [],
      "source": [
        "quantize_train, quant_train_info = tfds.load(DataSet, split='train + test[:75%]', with_info=True, as_supervised=True)\n",
        "filtered_quantize_train = quantize_train.filter(lambda x, y: filter_fn(y, class_list))\n",
        "\n",
        "resized_quantize_train = prepare(filtered_quantize_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VQBS_obQ8DF"
      },
      "outputs": [],
      "source": [
        "resized_quantize_train = resized_quantize_train.batch(BATCH_SIZE)\n",
        "h = q_aware_model.fit(resized_quantize_train, epochs=5, validation_data = resized_ds_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHskuvDaRBPb"
      },
      "outputs": [],
      "source": [
        "plt.plot(h.history['loss'])\n",
        "plt.plot(h.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFq4nw4PRDYT"
      },
      "outputs": [],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "quantized_tflite_model = converter.convert()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJbUyvBrRGBD"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(interpreter, test):\n",
        "    test_labels = []\n",
        "\n",
        "\n",
        "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "    \n",
        "    # Run predictions on every image in the \"test\" dataset.\n",
        "    prediction_digits = []\n",
        "    for i, test_example in enumerate(test):\n",
        "        if i % 1000 == 0:\n",
        "            print('Evaluated on {n} results so far.'.format(n=i))\n",
        "        test_labels.append(np.argmax(test_example[-1]))\n",
        "        test_image = test_example[0]\n",
        "        # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "        # the model's input data format.\n",
        "        #display(test_image.shape)\n",
        "        test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "        #test_image = np.expand_dims(test_image, axis=3).astype(np.float32)\n",
        "        #display(test_image.shape)\n",
        "        interpreter.set_tensor(input_index, test_image)\n",
        "        \n",
        "        # Run inference.\n",
        "        interpreter.invoke()\n",
        "        \n",
        "        # Post-processing: remove batch dimension and find the digit with highest\n",
        "        # probability.\n",
        "        output = interpreter.tensor(output_index)\n",
        "        digit = np.argmax(output()[0])\n",
        "        prediction_digits.append(digit)\n",
        "        \n",
        "    print('\\n')\n",
        "    # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "    prediction_digits = np.array(prediction_digits)\n",
        "    accuracy = (prediction_digits == test_labels).mean()\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ol3wMK2RIjN"
      },
      "outputs": [],
      "source": [
        "#Models obtained from TfLiteConverter can be run in Python with Interpreter.\n",
        "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
        "#Since TensorFlow Lite pre-plans tensor allocations to optimize inference, the user needs to call allocate_tensors() before any inference.\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "test_accuracy = evaluate_model(interpreter, test_as_np)\n",
        "\n",
        "print('Quant TFLite test_accuracy:', test_accuracy)\n",
        "#print('Quant TF test accuracy:', q_aware_model_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVX3TFb5RKeD"
      },
      "outputs": [],
      "source": [
        "MODEL_DIR = \"CadenceNet_Float\"\n",
        "model.save(MODEL_DIR, save_format=\"tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKEqxRdmRMOL"
      },
      "outputs": [],
      "source": [
        "!pip install -U tf2onnx==1.8.4\n",
        "!python -m tf2onnx.convert --saved-model /content/CadenceNet_Float/ --output /content/CadenceNetOriginal_Float.onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLDe9u9sRORk"
      },
      "outputs": [],
      "source": [
        "quant_file = \"/content/CadenceNetOriginal_QAT.tflite\"\n",
        "open(quant_file, \"wb\").write(quantized_tflite_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5iITOiiRP0M"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"Float model in Mb: \", os.path.getsize(\"/content/CadenceNetOriginal_Float.onnx\") / float(2**20))\n",
        "print(\"Quantized model in Mb: \", os.path.getsize(quant_file) / float(2**20))\n",
        "print(\"Float Model Accuracy: \", test_accuracy_Float)\n",
        "print(\"Quantized Model Accuracy: \", test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fOfhD35IW0f"
      },
      "outputs": [],
      "source": [
        "!pip install onnxruntime\n",
        "import onnxruntime as rt\n",
        "\n",
        "sess = rt.InferenceSession(\"/content/CadenceNetOriginal_Float.onnx\")\n",
        "input_name = sess.get_inputs()[0].name\n",
        "output_name = sess.get_outputs()[0].name\n",
        "x = np.random.random((1,IMG_SIZE,IMG_SIZE,NUM_CHANNELS))\n",
        "x = x.astype(np.float32)\n",
        "res = sess.run([output_name], {input_name: x})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_byw6-0Um7c"
      },
      "outputs": [],
      "source": [
        "indices = tf.convert_to_tensor([0, 1, 2])\n",
        "depth = 3\n",
        "indic = tf.convert_to_tensor([3, 5, 8])\n",
        "tf.math.multiply(indices, indic)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ftfq2j6b0CrC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}